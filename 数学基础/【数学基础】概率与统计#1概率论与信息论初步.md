>本系列内容介绍：
>
>![](https://i-blog.csdnimg.cn/direct/ba19606df94042ca938baf77c617285c.png)
>主要参考资料：
>
>《深度学习》[美]伊恩·古德菲洛 等 著
>
>《机器人数学基础》吴福朝 张铃 著
>
>文章为自学笔记，默认读者有一定的大学数学基础，仅供参考。

@[TOC](目录)
## 随机变量

**随机变量**是可以随机地取不同值的变量，是对可能状态的描述。变量和值用字体区分。随机变量$\mathrm x$的取值为$x$，向量值随机变量$\boldsymbol{\mathrm x}$的取值为$\boldsymbol x$。随机变量可以是离散的或者连续的。

## 概率分布

**概率分布**用来描述随机变量或一簇随机变量在每一个可能取值状态的可能性大小。

### 离散型随机变量和概率质量函数

离散型变量的概率分布可以用**概率质量函数**PMF来描述，用$P$表示。$\mathrm x\sim P(\mathrm x)$用来说明$\mathrm x$的分布，$P(\mathrm x=x)$或$P(x)$表示$\mathrm x$取到$x$的概率。

多个变量的概率分布被称为**联合概率分布**，$P(\mathrm x=x,\mathrm y=y)$或$P(x,y)$表示$\mathrm x=x$和$\mathrm y=y$同时发生的概率。

概率质量函数$P$必须满足下列条件：

- $P$的定义域是$\mathrm x$所有可能状态的集合。
- $\forall x\in\mathrm x,0\leqslant P(x)\leqslant1$
- $\displaystyle\sum_{x\in\mathrm x}P(x)=1$

### 连续型变量和概率密度函数

连续型变量的概率分布可以用**概率密度函数**PDF来描述，用$p$表示，其必须满足下列条件：

- $p$的定义域是$\mathrm x$所有可能状态的集合。
- $\forall x\in\mathrm x,p(x)\geqslant0$
- $\displaystyle\int p(x)\mathrm dx=1$

$p(x)$并没有直接对特定的状态给出概率，而是给出了落在面积为$\delta x$的无限小的区域内的概率为$p(x)\delta x$。对概率密度函数求积分可以获得点集的真实概率质量。

## 边缘概率

已知一组变量的联合概率分布，其中一个子集的概率分布被称为**边缘概率分布**。

二维随机变量的边缘概率分布计算如下：

$\forall x\in\mathrm x,P(x)=\displaystyle\sum_yP(x,y)$

$p(x)=\displaystyle\int p(x,y)\mathrm dy$

## 条件概率

某个事件在给定其他事件发生时出现的概率称为**条件概率**。给定$\mathrm x=x$，$\mathrm y=y$发生的概率为：

$P(\mathrm y=y|\mathrm x=x)=\displaystyle\frac{P(\mathrm y=y,\mathrm x=x)}{P(\mathrm x=x)}$

任何多维随机变量的联合概率分布都可以分解成只有一个变量的条件概率相乘的形式：

$P(\mathrm x^{(1)},\cdots,\mathrm x^{(n)})=P(\mathrm x^{(1)})\displaystyle\prod^n_{i=2}P(\mathrm x^{(i)}|\mathrm x^{(1)},\cdots,\mathrm x^{(i-1)})$

这个规则被称为概率的**链式法则**或者**乘法规则**。

## 独立性和条件独立性

如果两个随机变量同时取到某值的概率是它们各自取值的概率的乘积，则称它们是**相互独立**的：

$\forall x\in\mathrm x,y\in\mathrm y,p(\mathrm x=x,\mathrm y=y)=p(\mathrm x=x)p(\mathrm y=y)$

如果关于$\mathrm x$和$\mathrm y$的条件概率分布对于$z$的每一个值都可以写成乘积的形式，那么这两个随机变量$\mathrm x$和$\mathrm y$在给定随机变量$\mathrm z$时是**条件独立**的：

$\forall x\in\mathrm x,y\in\mathrm y,z\in\mathrm z,p(\mathrm x=x,\mathrm y=y|\mathrm z=z)=p(\mathrm x=x|\mathrm z=z)p(\mathrm y=y|\mathrm z=z)$

## 期望、方差和协方差

函数$f(x)$关于某分布$P(\mathrm x)$的**期望**是指当$x$由$P$产生，作用于$f$时，$f(x)$的平均值：

$\mathbb E_{\mathrm x\sim P}[f(x)]=\displaystyle\sum_xP(x)f(x)$

$\mathbb E_{\mathrm x\sim p}[f(x)]=\displaystyle\int p(x)f(x)\mathrm dx$

当概率分布在上下文指明时，期望可简记为$\mathbb E_\mathrm x[f(x)]$。如果随机变量很明确，期望可进一步简记为$\mathbb E[f(x)]$。默认$\mathbb E[\cdot]$表示对方括号内所有随机变量的值求平均，当没有歧义时还可以省略方括号。

期望是线性的：

$\mathbb E_\mathrm x[\alpha f(x)+\beta g(x)]=\alpha\mathbb E_\mathrm x[f(x)]+\beta\mathbb E_\mathrm x[g(x)]$

当我们对$x$依据它的概率分布进行采样时，随机变量$\mathrm x$的函数值呈现的差异大小可以用**方差**衡量：

$\mathrm{Var}(f(x))=\mathbb E[(f(x)-\mathbb E[f(x)])^2]$

方差的平方根称为**标准差**。

**协方差**给出了两个变量线性相关性的强度以及这些变量的尺度：

$\mathrm{Cov}(f(x),g(y))=\mathbb E[(f(x)-\mathbb E[f(x)])(g(y)-\mathbb E[g(y)])]$

协方差的绝对值如果很大，意味着变量值变化很大，并且同时距离各自的均值很远。正的协方差说明两个变量倾向于同时取得较大的值，负的协方差说明一个变量取到较大的值时，另一个变量倾向于取得较小的值。

零协方差排除了两个变量的线性关系，而独立性排除了线性关系和非线性关系，要求更强。

随机向量$\boldsymbol x\in\mathbb R^n$的**协方差矩阵**是一个$n$阶方阵，并满足：

$\mathrm{Cov}(\boldsymbol{\mathrm x})_{i,j}=\mathrm{Cov}(\mathrm x_i,\mathrm x_j)$

协方差矩阵的对角元素是方差：

$\mathrm{Cov}(\mathrm x_i,\mathrm x_i)=\mathrm{Var}(\mathrm x_i)$

## 常用概率分布

### Bernoulli分布

**Bernoulli分布**是单个二值随机变量的分布，具有如下性质：

$P(\mathrm x=1)=\phi$

$P(\mathrm x=0)=1-\phi$

$P(\mathrm x=x)=\phi^x(1-\phi)^{(1-x)}$

$\mathbb E_\mathrm x[\mathrm x]=\phi$

$\mathrm{Var}_\mathrm x(\mathrm x)=\phi(1-\phi)$

### Multinoulli分布

**Multinoulli分布**或者**范畴分布**是指在具有$k$个不同状态的单个离散型随机变量上的分布，其中$k$是一个有限值。Moutinoulli分布由向量$\boldsymbol p\in[0,1]^{k-1}$参数化，其中每个分量$p_i$表示第$i$个状态的概率。最后的第$k$个概率由$1-\boldsymbol 1^\top\boldsymbol p$给出。

Multinoulli分布常用于表示对象分类的分布，通常不需要计算期望和方差。

### 高斯分布

实数最常用的分布是**正态分布**，也称为**高斯分布**：

$\mathcal N(x;\mu,\sigma^2)=\displaystyle\sqrt\frac1{2\pi\sigma^2}\exp\left(-\frac1{2\sigma^2}(x-\mu)^2\right)$

参数$\mu$为期望，$\sigma^2$为方差。

当我们需要经常对不同参数下的概率密度求值时，一种更高效的参数化分布的方式是使用方差的倒数$\beta\in(0,\infty)$来控制分布的**精度**：

$\mathcal N(x;\mu,\beta)=\displaystyle\sqrt\frac{\beta}{2\pi}\exp\left(-\frac12\beta(x-\mu)^2\right)$

当我们缺乏关于某个实数上分布的先验知识时，正态分布是默认的比较好的选择：第一，**中心极限定理**说明很多独立随机变量的和近似服从正态分布；第二，在具有相同方差的所有可能的概率分布中，正态分布在事实上具有最大的不确定性。

推广到$\mathbb R^n$空间的正态分布被称为**多维正态分布**：

$\mathcal N(\boldsymbol x;\boldsymbol\mu,\boldsymbol\Sigma)=\displaystyle\sqrt\frac1{(2\pi)^n\mathrm{det}(\boldsymbol\Sigma)}\exp\left(-\frac12(\boldsymbol x-\boldsymbol\mu)^\top\boldsymbol\Sigma^{-1}(\boldsymbol x-\boldsymbol\mu)\right)$

向量值$\boldsymbol\mu$表示分布的均值，$\boldsymbol\Sigma$给出了分布的协方差矩阵。一种更高效的参数化分布方式是使用$\boldsymbol\Sigma$的逆矩阵**精度矩阵**$\boldsymbol\beta$进行替代：

$\mathcal N(\boldsymbol x;\boldsymbol\mu,\boldsymbol\beta^{-1})=\displaystyle\sqrt\frac{\mathrm{det}(\boldsymbol\beta)}{(2\pi)^n}\exp\left(-\frac12(\boldsymbol x-\boldsymbol\mu)^\top\boldsymbol\beta(\boldsymbol x-\boldsymbol\mu)\right)$

我们常常把协方差矩阵限制为一个对角阵。一个更简单的版本是**各向同性**高斯分布，它的协方差矩阵是一个标量乘以单位阵。

### 指数分布和Laplace分布

在深度学习中，为了实现一个在$x=0$点处取得边界点的分布，我们可以使用**指数分布**：

$p(x;\lambda)=\lambda\boldsymbol1_{x\geqslant0}\exp(-\lambda x)$

**Laplace分布**允许我们在任意一点$\mu$处设置概率质量的峰值：

$\mathrm{Laplace}(x;\mu,\gamma)=\displaystyle\frac1{2\gamma}\exp\left(-\frac{|x-\mu|}\gamma\right)$

### Dirac分布和经验分布

在一些情况下，我们希望概率分布中的所有质量都集中在一个点上，这可以通过**Dirac delta 函数**$\delta(x)$定义概率密度函数来实现：

$p(x)=\delta(x-\mu)$

Dirac delta函数是依据积分性质定义的**广义函数**，其在除了$0$以外的所有点的值都为$0$，但是积分为$1$。

Dirac分布经常作为**经验分布**的一个组成部分出现：

$\hat p(\boldsymbol x)=\displaystyle\frac1m\sum^m_{i=1}\delta(\boldsymbol x-\boldsymbol x^{(i)})$

经验分布将概率密度$\displaystyle\frac1m$赋给$m$个点$\boldsymbol x^{(1)},\cdots,\boldsymbol x^{(m)}$中的每一个，这些点是给定的数据集或者采样的集合。

### 分布的混合

通过一些简单的概率分布来定义新的概率分布的通用的组合方法是构造**混合分布**。混合分布由一些组件分布构成，每次实验样本是由哪个组件分布产生的取决于从一个Multinoulli分布中采样的结果：

$P(\mathrm x)=\displaystyle\sum_iP(c=i)P(\mathrm x|c=i)$

其中$P(c)$是对各组件的一个Multinoulli分布，组件标识变量$c$是一个我们不能直接观测到的**潜变量**。

一个非常强大且常见的混合模型是**高斯混合模型**，它的组件$p(\mathrm x|c=i)$是高斯分布，每个组件都有各自的参数，也可以有更多限制，例如参数共享。

除了均值和协方差，高斯混合模型的参数指明了给每个组件$i$的**先验概率**：

$\alpha_i=P(c=i)$

“先验”一词表明了在观测到$\mathrm x$之前传递给模型关于$c$的信念。相应地，$P(c|\boldsymbol x)$是**后验概率**。

高斯混合模型时概率密度的**万能近似器**，任何平滑的概率密度都可以用具有足够多组件的高斯混合模型以任意精度来逼近。

## 贝叶斯规则

**贝叶斯规则**给出了两个事件条件概率间的关系，其实际内涵为支持某项属性的事件发生得愈多，则该属性成立的可能性就愈大：

$P(\mathrm x|\mathrm y)=\displaystyle\frac{P(\mathrm x)P(\mathrm y|\mathrm x)}{P(\mathrm y)}$

$P(\mathrm y)$通常计算如下：

$P(\mathrm y)=\displaystyle\sum_xP(\mathrm y|x)P(x)$

## 信息论

推荐学习：

[“交叉熵”如何做损失函数？打包理解“信息量”、“比特”、“熵”、“KL散度”、“交叉熵”_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV15V411W7VB/?spm_id_from=333.337.search-card.all.click&vd_source=14840859b7706d40ba924de91194c603)

**信息论**告诉我们如何对消息设计最优编码和计算消息的期望长度，其中的一些关键思想可用来描述概率分布或者量化概率分布之间的相似性。

量化信息的一些基本想法如下：

- 非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件应该没有信息量。
- 较不可能发生的事件具有更高的信息量。
- 独立事件应具有增量的信息，例如投掷的硬币两次正面朝上传递的信息量应该是投掷一次硬币正面朝上的信息量的两倍。

为了满足上述性质，我们定义一个事件$\mathrm x=x$的**自信息**为：

$I(x)=-\log P(x)$

$I(x)$的单位在以$e$为底数时是**奈特**，以$2$为底数时是**比特**或**香农**。

**香农熵**对整个概率分布中的不确定性总量进行量化：

$H(\mathrm x)=\mathbb E_{\mathrm x\sim P}[I(x)]=-\mathbb E_{\mathrm x\sim P}[\log P(x)]$

当$\mathrm x$是连续的，香农熵被称为**微分熵**。

如果一个随机变量$\mathrm x$有两个单独的概率分布$P(\mathrm x)$和$Q(\mathrm x)$，可以使用**KL散度**来衡量这两个分布的差异：

$D_{\mathrm{KL}}(P||Q)=\mathbb E_{\mathrm x\sim P}\left[\displaystyle\log\frac{P(x)}{Q(x)}\right]=\mathbb E_{\mathrm x\sim P}[\log P(x)-\log Q(x)]$

KL散度是非负的，因此经常被用作分布之间的某种距离，但它不是对称的，以不同的分布为基准，求得的值不一定一样。

通过KL散度和香农熵，我们定义了**交叉熵**：

$H(P,Q)=H(P)+D_{\mathrm{KL}}(P||Q)=-\mathbb E_{\mathrm x\sim P}\log Q(x)$

交叉熵越小，KL散度越小，因此交叉熵成为了机器学习常用的损失函数之一。

## 结构化概率模型

机器学习经常涉及到在非常多的随机变量上的概率分布，使用单个函数来描述整个联合概率分布是非常低效的。

将概率分布分解成许多因子的乘积形式可以极大地减少用来描述一个分布的参数数量。假设有三个随机变量$\mathrm a$、$\mathrm b$和$\mathrm c$，并且$\mathrm a$影响$\mathrm b$，$\mathrm b$影响$\mathrm c$，$\mathrm a$和$\mathrm c$在给定$\mathrm b$时条件独立，由此：

$p(\mathrm a,\mathrm b,\mathrm c)=p(\mathrm a)p(\mathrm b|\mathrm a)p(\mathrm c|\mathrm b)$

每个因子使用的参数数目是其变量数目的指数倍，如果我们能够找到一种使每个因子分布具有更少变量的分解方法就能极大地降低表示联合分布的成本。可以用图论中的图$\mathcal G$来描述这种分解，称为**结构化概率模型**或**图模型**。

**有向**模型使用带有有向边的图，它们用条件概率分布来表示分解。有向模型对于分布中的每一个随机变量$\mathrm x_i$都包含着一个影响因子，这个组成$\mathrm x_i$条件概率的影响因子被称为$\mathrm x_i$的父节点，记为$Pa_\mathcal G(x_i)$：

$p(\boldsymbol{\mathrm x})=\displaystyle\prod_ip(\mathrm x_i|Pa_\mathcal G(\mathrm x_i))$

![](https://i-blog.csdnimg.cn/direct/a874a14728b0407a9297ca54e820a084.png)


**无向**模型使用带有无向边的图，它们将分解表示成并非概率分布的一组函数。$\mathcal G$中任何满足两两之间有边连接的顶点的集合被称为团，每个团$\mathcal C^{(i)}$都伴随着一个因子$\phi^{(i)}(\mathcal C^{(i)})$，每个因子的输出都必须是非负的。随机变量的联合概率与所有这些因子的乘积成比例，归一化常数$Z$被定义为$\phi$函数乘积的所有状态的求和或积分：

$p(\boldsymbol{\mathrm x})=\displaystyle\frac1Z\prod_i\phi^{(i)}(\mathcal C^{(i)})$

![](https://i-blog.csdnimg.cn/direct/1c1b0e02cfd2483ebd49bcb5015ba926.png)

