> 主要参考学习资料：
> 
> 《动手学深度学习》阿斯顿·张 等 著
> 
> 【动手学深度学习 PyTorch版】哔哩哔哩@跟李牧学AI

为了进一步提高长线学习的效率，该系列从本章开始将舍弃原始教材的代码部分，专注于理论和思维的提炼，系列名也改为“深度学习”消除误导性。在学习中将理论与实践紧密结合固然有其好处，但在所学知识的广度面前，先铺垫广泛的理论基础，再根据最终实践的目标筛选出需要得到深化的理论知识可能是更有效的策略。

@[TOC](目录)

**概述**

- **序列模型**是处理序列信息的模型。
- 处理文本序列信息的**语言模型**存在其独有的挑战。
- **循环神经网络**通过**隐状态**存储过去的信息。
- **通过时间反向传播**是反向传播在循环神经网络中的特定应用。
- **截断时间步**和**梯度裁剪**可以缓解循环神经网络的梯度消失与爆炸问题。

# 序列模型

卷积神经网络可以有效地处理空间信息，而**循环神经网络**（RNN）则可以很好地处理序列信息。循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。

音乐、语音、文本和视频都是序列信息，如果它们的序列被重排，就会失去原本的意义。

处理序列数据需要统计工具和新的深度神经网络架构。假设对于一组序列数据，在**时间步**$t\in\mathbb Z^+$时观察到的数据为$x_t$，则预测$x_t$的途径为：

$x_t\sim P(x_t|x_{t-1},\cdots,x_1)$

有效估计$P(x_t|x_{t-1,\cdots,x_1})$的策略归结为以下两种：

第一种，假设现实情况下观测所有之前的序列是不必要的，只需要满足某个长度为$\tau$的时间跨度，即观测序列$x_{t-1},\cdots,x_{t-\tau}$，至少在$t>\tau$时。此时输入的数量固定，可以使用之前的深度网络，这种模型被称为**自回归模型**。

第二种，引入潜变量$h(t)$表示过去的信息，并且同时更新$\hat x_t=P(x_t|h_t)$和$h_t=g(h_{t-1},x_{t-1})$。由于$h(t)$从未被观测到，这类模型被称为**隐变量自回归模型**，如下图所示：

![](https://i-blog.csdnimg.cn/direct/4f7061ce06c948f7b60aa72de07c711b.png)


# 文本预处理

文本是最常见的序列数据之一。文本常见的预处理步骤通常包括：

1. 将文本作为字符串加载到内存中；
2. 将字符串拆分为词元（如单词和字符）；
3. 建立一个词表，将拆分的词元映射到数字索引；
4. 将文本转换为数字索引序列，以便模型操作。

**词元**是文本的基本单位，可以是单词或字符。但模型需要的输入是数字而不是字符串，因此需要构建一个字典，也称**词表**，来将字符串类型的词元映射到从0开始的数字索引中。统计一个文本数据集中出现的所有不重复的词元，得到的结果称为**语料库**。词表根据语料库中每个词元出现的频率为其分配一个数字索引，很少出现的词元则通常被移除以降低复杂度。除此之外，未知词元$\texttt{'<unk>'}$用于映射语料库中不存在的词元，填充词元$\texttt{'<pad>'}$用于填充长度不足的序列使输入序列具有相同的长度，序列开始词元$\texttt{'<bos>'}$和序列结束词元$\texttt{'<eos>'}$定义序列的开始与结束。

# 语言模型

## 马尔可夫模型与n元语法

在自回归模型中，我们使用$x_{t-1},\cdots,x_{t-\tau}$而非$x_{t-1},\cdots,x_1$预测$x_t$。当预测的状态只与其前$n$个时间步的状态有关，我们称其满足$n$阶马尔可夫性质，相应的模型被称为$n$阶马尔可夫模型。阶数越高，对应的依赖关系链就越长。根据马尔可夫性质可以推导出许多应用于序列建模的近似公式：

$P(x_1,x_2,x_3,x_4)=P(x_1)P(x_2)P(x_3)P(x_4)$

$P(x_1,x_2,x_3,x_4)=P(x_1)P(x_2|x_1)P(x_3|x_2)P(x_4|x_3)$

$P(x_1,x_2,x_3,x_4)=P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)P(x_4|x_3,x_2)$

使用$1$阶、$2$阶、$3$阶马尔可夫性质的概率公式被称为**一元语法**、**二元语法**和**三元语法**模型。

## 自然语言统计

自然语言的统计结果往往符合一些规律。

首先，词频最高的词很多为虚词（the、and、of等），这些词通常被称为**停用词**，在做文本分析时可以被过滤掉。但它们本身有一定意义，因此模型仍然使用它们。

还有一个问题是词频衰减的速度相当快，以下是H.G.Wells的小说*The Time Machine*的词频图：

![](https://i-blog.csdnimg.cn/direct/3a3dcb31ae2443aea3a0a02ac1590766.png)


除去前几个单词，剩余的单词与其词频的变化规律大致遵循双对数坐标图上的一条直线。这意味着单词的频率满足**齐普夫定律**，即第$i$个最常用单词的频率$n_i$满足

$n_i\propto\displaystyle\frac1{i^\alpha}$

其等价于

$\log n_i=-\alpha\log i+c$

其中$\alpha$是描述分布的指数，$c$是常数。这意味着较少的单词拥有极高的词频，而大部分单词的词频与之相比十分地低。

在二元语法（bigram）和三元语法（trigram）的情况下，即两个单词和三个单词构成的序列，它们的分布也在一定程度上遵循齐普夫定律：

![](https://i-blog.csdnimg.cn/direct/21b61426980c4415854a606a9ab2adb2.png)


随着$n$元语法中$n$的增大，$n$个单词的序列的最高词频的衰减也十分迅速，且越来越多的单词序列只会出现$1$次。

## 学习语言模型

假设长度为$T$的文本序列中的词元依次为$x_1,x_2,\cdots,x_T$，于是$x_t(1\leqslant t\leqslant T)$可以被认为是文本序列在时间步$t$处的观测或标签。在给定这样的文本序列时，**语言模型**的目标是估计序列的联合概率

$$
P(x_1,x_2,\cdots,x_T)=\displaystyle\prod^T_{t=1}P(x_t|x_1,\cdots,x_{t-1})
$$

例如，包含$4$个单词的一个文本序列是

$$
P(\mathrm{deep},\mathrm{learning},\mathrm{is},\mathrm{fun})=P(\mathrm{deep})P(\mathrm{learning}|\mathrm{deep})P(\mathrm{is}|\mathrm{deep},\mathrm{learning})P(\mathrm{fun}|\mathrm{deep},\mathrm{learning},\mathrm{is})
$$

为了训练语言模型，我们需要计算单词出现的概率以及给定前面几个单词后出现某个单词的条件概率。训练数据集中单词的概率可以根据给定单词的相对词频计算，例如通过统计单词“deep”在数据集中出现的次数，然后除以整个语料库中的单词数来得到估计值$\hat P(\mathrm{deep})$。这对频繁出现的单词效果不错。

接下来我们尝试估计

$$
\hat P(\mathrm{learning}|\mathrm{deep})=\displaystyle\frac{n(\mathrm{deep},\mathrm{learning})}{n(\mathrm{deep})}
$$

其中$n(x)$和$n(x,x')$分别为单个单词和连续单词对出现的次数。但是根据齐普夫定律可想而知，连续单词对“deep learning”出现的频率低得多，要想找到足够的出现次数来获得准确的估计并不容易。而对于$3$个或更多单词的组合，情况会变得更糟。

一种常见的应对小概率和零概率问题的策略是**拉普拉斯平滑**，它在所有计数中添加一个小常量$\epsilon$：

$\hat P(x)=\displaystyle\frac{n(x)+\epsilon_1/m}{n+\epsilon_1}$

$\hat P(x'|x)=\displaystyle\frac{n(x,x')+\epsilon_2\hat P(x)}{n(x)+\epsilon_2}$

其中$m$为训练集中不重复单词的数量，不同变量数目的条件概率估计所用的$\epsilon$都是超参数。但这种方法很容易变得无效，原因如下：

- 我们需要存储所有单词和单词序列的计数。
- 该方法忽视了单词的意思和具有跨度的上下文关系。
- 长单词序列中的大部分是没出现过的，因而无法区分它们。

循环神经网络及其变体，和后续介绍的其他序列模型将克服这些问题。

# 循环神经网络

对于$n$元语法模型，想要基于更大的时间跨度作出预测，只有增大$n$，代价是参数将指数级增长，因为每个不重复的单词都需要存储在所有长度小于$n$的单词序列之后其出现的条件概率。使用隐变量自回归模型则可以对参数进行压缩：

$P(x_t|x_{t-1},\cdots,x_1)\approx P(x_t|h_{t-1})$

其中$h_{t-1}$是**隐状态**，也称为**隐藏变量**，它会在每个时间步以新的输入$x_t$和前一时刻的隐状态$h_{t-1}$更新自己：

$h_t=f(x_t,h_{t-1})$

尽管我们不知道隐状态具体存储了怎样的信息，但就公式来看，它在每个时间步确实是由所有之前的输入共同决定的。缺乏直观的可解释性是深度学习的普遍特点。

## 有隐状态的循环神经网络

**循环神经网络**（RNN）是具有隐状态的神经网络。

假设有$n$个序列样本的小批量，在时间步$t$，小批量输入$\mathbf X_t\in\mathbb R^{n\times d}$的每一行对应一个序列时间步$t$处的一个样本。接下来，我们用$\mathbf H_t\in\mathbb R^{n\times h}$表示时间步$t$的隐藏变量，并保存了前一个时间步的隐藏变量$\mathbf H_{t-1}$和利用它更新自身的权重参数$\mathbf W_{hh}\in\mathbb R^{h\times h}$。接收到输入时，RNN会先更新隐藏变量：

$\mathbf H_t=\phi(\mathbf X_t\mathbf W_{xh}+\mathbf H_{t-1}\mathbf W_{hh}+\mathbf b_h)$

再根据新的隐藏变量计算输出层的输出：

$\mathbf O_t=\mathbf H_t\mathbf W_{hq}+\mathbf b_q$

可见，隐藏变量类似于多层感知机中隐藏层的输出，但不同的是它在计算时多出了$\mathbf H_{t-1}\mathbf W_{hh}$一项，不再仅由当前的输入决定。

对于更多的时间步，RNN不会引入新的参数，只会更新已有的参数，以此维持稳定的参数开销。同时，和普通的自回归模型相比，RNN有固定数量的输入，不会因考虑的时间步而改变。

将$\mathbf X_t\mathbf W_{xh}+\mathbf H_{t-1}\mathbf W_{hh}$写成分块矩阵乘法可得：

$\mathbf X_t\mathbf W_{xh}+\mathbf H_{t-1}\mathbf W_{hh}=\begin{bmatrix}\mathbf X_t,\mathbf H_{t-1}\end{bmatrix}\begin{bmatrix}\mathbf W_{xh}\\\mathbf W_{hh}\end{bmatrix}$

因此该计算相当于将$\mathbf X_t$和$\boldsymbol H_{t-1}$按行连接，将$\mathbf W_{xh}$和$\mathbf W_{hh}$按列连接再相乘。由此有隐状态的循环神经网络的计算逻辑如下图所示：

![](https://i-blog.csdnimg.cn/direct/371c8bb4ca1a4219ad4cf2ce26de5e46.png)


## 困惑度

由于语言模型和分类模型在本质上有共通之处，输出都是离散的，因此可以引入交叉熵衡量其质量。一个更好的语言模型应该在预测中对序列下一个标签词元给出更高的概率，对序列中所有$n$个词元的交叉熵损失求平均值得到：

$\displaystyle\frac1n\sum^n_{i=1}-\log P(x_i|x_{i-1},\cdots,x_1)$

由于历史原因，自然语言处理领域更常使用**困惑度**，它是对上式进行指数运算的结果：

$PP=\exp\left(-\displaystyle\frac1n\sum^n_{i=1}\log P(x_i|x_{i-1},\cdots,x_1)\right)$

- 在最好的情况下，模型对标签词元的概率估计总是$1$，此时困惑度为$1$。
- 在最坏的情况下，模型对标签词元的概率估计总是$0$，此时困惑度为正无穷。
- 在基线上，模型对词表中所有不重复词元的概率估计均匀分布，此时困惑度为不重复词元的数量。

# 通过时间反向传播

**通过时间反向传播**（BPTT）是循环神经网络中反向传播技术的一个特定应用。

## 循环神经网络的梯度分析

我们先从循环神经网络的简化模型开始，将时间步$t$的隐状态表示为$h_t$，输入表示为$x_t$，输出表示为$o_t$，并使用$w_h$和$w_o$表示隐藏层（拼接后）和输出层的权重。则每个时间步的隐状态和输出可写为：

$h_t=f(x_t,h_{t-1},w_h)$

$o_t=g(h_t,w_o)$

对于前向传播，我们有目标函数$L$评估所有$T$个时间步内输出$o_t$和对应的标签$y_t$之间的差距：

$L(x_1,\cdots,x_T,y_1,\cdots,y_T,w_h,w_o)=\displaystyle\frac1T\sum^T_{t=1}l(y_t,o_t)$

对于反向传播，目标函数$L$对于参数$w_h$的梯度按照链式法则有：

$$
\begin{equation}\begin{split}\displaystyle\frac{\partial L}{\partial w_h}&=\frac1T\sum^T_{t=1}\frac{\partial l(y_t,o_t)}{\partial w_h}\\&=\frac1T\sum^T_{t=1}\frac{\partial l(y_t,o_t)}{\partial o_t}\frac{\partial g(h_t,w_o)}{\partial h_t}\frac{\partial h_t}{\partial w_h}\end{split}\end{equation}
$$

乘积的第一项和第二项很容易计算，而第三项$\displaystyle\frac{\partial h_t}{\partial w_h}$既依赖$h_{t-1}$又依赖$w_h$，而$h_{t-1}$也依赖$w_h$，需要循环计算参数$w_h$对$h_t$的影响：

$$
\begin{equation}\begin{split}\displaystyle\frac{\partial h_t}{\partial w_h}&=\frac{\partial f(x_t,h_{t-1},w_h)}{\partial w_h}+\frac{\partial f(x_t,h_{t-1},w_h)}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial w_h}\\&=\frac{\partial f(x_t,h_{t-1},w_h)}{\partial w_h}+\sum^{t-1}_{i=1}\left(\prod^t_{j=i+1}\frac{\partial f(x_j,h_{j-1},w_h)}{\partial h_{j-1}}\right)\frac{\partial f(x_i,h_{i-1},w_h)}{\partial w_h}\end{split}\end{equation}
$$

当$t$很大时，计算链条会变得很长，需要采取办法解决这一问题。

### 1.截断时间步

截断时间步在$\tau$步后截断上式的求和运算，只将求和终止到$\displaystyle\frac{\partial h_{t-\tau}}{\partial w_h}$来近似实际梯度。这使得模型主要侧重于短期影响而非长期影响，变得更简单但也更稳定。

### 2.随机截断

随机截断通过随机变量序列$\xi_t$实现在随机时间步后截断。$\xi_t$预先确定好一个值$0\leqslant\pi_t\leqslant1$，再根据如下两点分布生成序列：

$P(\xi_t=0)=1-\pi_t$

$P(\xi_t=\pi_t^{-1})=\pi_t$

于是我们用如下式子来替换$\displaystyle\frac{\partial h_t}{\partial w_h}$的计算：

$z_t=\displaystyle\frac{\partial f(x_t,h_{t-1},w_h)}{\partial w_h}+\xi_t\frac{\partial f(x_t,h_{t-1},w_h)}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial w_h}$

在递归计算的过程中，$\xi_t$会随机在某个时间步取到$0$，进而截断后续的计算，同时之前计算的权重会随着递归的次数而增加。最终效果是短序列梯度出现的概率高，而长序列梯度出现的概率低，但长序列得到的权重更高。根据$\mathbb E[\xi_t]=1$和期望的线性性质可以得到$\mathbb E[z_t]=\displaystyle\frac{\partial h_t}{\partial w_h}$，因此该估计是无偏的。

随机截断在实践中可能并不比常规截断好，原因有三：

- 在对过去若干时间步进行反向传播后，观测结果足以捕获实际的依赖关系。
- 随机变量带来的方差抵消了时间步越多梯度越精确的效果。
- 局部性先验的小范围交互模型更匹配数据的本质。

## 通过时间反向传播的细节

从简化模型扩展到对单个样本的运算，对于时间步$t$，设单个样本的输入及其对应的标签分别为$\boldsymbol x_t\in\mathbb R^d$和$y_t$，计算隐状态$\boldsymbol h_t\in\mathbb R^h$和输出$o_t\in\mathbb R^q$的公式为：

$\boldsymbol h_t=\boldsymbol W_{hx}\boldsymbol x_t+\boldsymbol W_{hh}\boldsymbol h_{t-1}$

$\boldsymbol o_t=\boldsymbol W_{qh}\boldsymbol h_t$

其中权重参数为$\mathbf W_{ht}\in\mathbb R^{h\times d}$、$\mathbf W_{hh}\in\mathbb R^{h\times h}$和$\mathbf W_{qh}\in\mathbb R^{q\times h}$。

用$l(\mathbf o_t,y_t)$表示时间步$t$处的损失函数，则目标函数的总体损失为：

$L=\displaystyle\frac1T\sum^T_{t=1}l(\mathbf o_t,y_t)$

接下来沿箭头所指反方向遍历循环神经网络的计算图：

![](https://i-blog.csdnimg.cn/direct/915a342e330648d6b7ceb688d684cdff.png)


首先，在任意时间步$t$，目标函数关于模型输出的微分计算为：

$\displaystyle\frac{\partial L}{\partial\boldsymbol o_t}=\frac{\partial l(\boldsymbol o_t,y_t)}{T\cdot\partial\boldsymbol o_t}\in\mathbb R^q$

目标函数对$\mathbf W_{qh}$的梯度依赖于$\boldsymbol o_t$，根据链式法则有（prod表示链式法则中的乘法运算）：

$\displaystyle\frac{\partial L}{\partial\mathbf{W}_{qh}}=\sum_{t=1}^T\text{prod}\left(\frac{\partial L}{\partial\mathbf{o}_t},\frac{\partial\mathbf{o}_t}{\partial\mathbf{W}_{qh}}\right)=\sum_{t=1}^T\frac{\partial L}{\partial\mathbf{o}_t}\mathbf{h}_t^\top\in\mathbb R^{q\times h}$

在最终时间步$T$，目标函数对$\mathbf h_T$的梯度仅依赖于$\mathbf o_T$：

$\displaystyle\frac{\partial L}{\partial\mathbf{h}_T}=\text{prod}\left(\frac{\partial L}{\partial\mathbf{o}_T},\frac{\partial\mathbf{o}_T}{\partial\mathbf{h}_T}\right)=\mathbf{W}_{qh}^\top\frac{\partial L}{\partial\mathbf{o}_T}\in\mathbb R^h$

但目标函数对其余时间步下$\mathbf h_t$的梯度依赖于$\mathbf o_t$和$\mathbf h_{t+1}$，此时需要进行递归运算：

$\displaystyle\frac{\partial L}{\partial\mathbf{h}_t}=\text{prod}\left(\frac{\partial L}{\partial\mathbf{h}_{t+1}},\frac{\partial\mathbf{h}_{t+1}}{\partial\mathbf{h}_t}\right)+\text{prod}\left(\frac{\partial L}{\partial\mathbf{o}_t},\frac{\partial\mathbf{o}_t}{\partial\mathbf{h}_t} \right) = \mathbf{W}_{hh}^\top\frac{\partial L}{\partial\mathbf{h}_{t+1}}+\mathbf{W}_{qh}^\top\frac{\partial L}{\partial\mathbf{o}_t}$

将该递归式展开可得：

$\displaystyle\frac{\partial L}{\partial\mathbf{h}_t}=\sum_{i=t}^T{\left(\mathbf{W}_{hh}^\top\right)}^{T-i}\mathbf{W}_{qh}^\top\frac{\partial L}{\partial\mathbf{o}_{T+t-i}}$

上式出现了$\mathbf W_{hh}^\top$的幂运算，当指数非常大时，幂中小于$1$的特征值会消失，大于$1$的特征值会发散，表现形式为梯度消失和梯度爆炸。一种解决方法是截断时间步，下一章将介绍缓解这一问题的更复杂的序列模型。

最后，目标函数对隐藏层模型参数$\mathbf W_{xh}$和$\mathbf W_{hh}$的梯度依赖于所有的$\mathbf h_t$，根据链式法则有：

$\displaystyle\frac{\partial L}{\partial\mathbf{W}_{hx}}=\sum_{t=1}^T\text{prod}\left(\frac{\partial L}{\partial\mathbf{h}_t},\frac{\partial\mathbf{h}_t}{\partial\mathbf{W}_{hx}}\right)\sum_{t=1}^T \frac{\partial L}{\partial\mathbf{h}_t}\mathbf{x}_t^\top$

$\displaystyle\frac{\partial L}{\partial \mathbf{W}_{hh}}=\sum_{t=1}^T\text{prod}\left(\frac{\partial L}{\partial\mathbf{h}_t}, \frac{\partial\mathbf{h}_t}{\partial \mathbf{W}_{hh}}\right)= \sum_{t=1}^T \frac{\partial L}{\partial \mathbf{h}_t} \mathbf{h}_{t-1}^\top$

## 梯度裁剪

除了专门针对BPTT的截断时间步，还有一种更普适的修复梯度爆炸的方法，称为**梯度裁剪**。

最基本的梯度裁剪在反向传播的过程中对梯度进行限制，在梯度的范数超过某个阈值时，将其按比例缩小，其公式为：

$\boldsymbol g\leftarrow\min\left(\displaystyle1,\frac\theta{||\boldsymbol g||}\right)\boldsymbol g$

通过这样做，梯度范数将永远不会超过$\theta$。
