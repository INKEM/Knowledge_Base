> 主要参考学习资料：
> 
> 《动手学深度学习》阿斯顿·张 等 著
> 
> 【动手学深度学习 PyTorch版】哔哩哔哩@跟李牧学AI

**概述**

- **门控循环单元**和**长短期记忆网络**利用门控机制实现对序列输入的选择性记忆。
- **深度循环神经网络**堆叠多个循环神经网络层以实现更强的表达能力和特征提取能力。
- **双向循环神经网络**同时捕捉过去和未来两个方向的依赖关系。
- **序列到序列**类任务的输入和输出均为可变长度序列，主要使用**编码器-解码器架构**。
- **束搜索**是一种在解码器选择输出序列时兼顾精确度和计算量的搜索方法。


@[TOC](目录)

# 门控循环单元（GRU）

在序列数据中，通常不是所有的信息都同等重要，我们需要提炼出关键信息，过滤掉无关的内容，门控机制则受此启发而来。当判断出重要信息时，信息从开启的门流出，当信息可以被遗忘时，则被关闭的门隔断。最早的相关方法为长短期记忆网络（LSTM），而**门控循环单元**（GRU）作为其简化变体，先行学习更易于理解。

## 重置门和更新门

![](https://i-blog.csdnimg.cn/direct/59d328f2ef064675bde122e41d411efe.png)


GRU包含重置门和更新门两个门控，它们都以当前时间步的输入和前一个时间步的隐状态作为输入，并通过sigmoid激活函数将各自的输出压缩到区间$(0,1)$中，而输出将作为比例系数应用到后续计算中。换言之，重置门和更新门先根据输入给出筛选信息的方法，而筛选的实际操作是在之后进行的。

对于在时间步$t$的小批量输入$\mathbf{X}_t \in \mathbb{R}^{n \times d}$（$n$个样本，每个样本$d$个输入），前一个时间步的隐状态$\mathbf H_{t-1}\in\mathbb R^{n\times h}$（$h$个隐藏单元），重置门$\mathbf R_t\in\mathbb R^{n\times h}$和更新门$\mathbf Z_t\in\mathbb R^{n\times h}$的计算如下：

$\mathbf{R}_t=\sigma(\mathbf{X}_t\mathbf{W}_{xr}+\mathbf{H}_{t-1}\mathbf{W}_{hr}+\mathbf{b}_r)$

$\mathbf{Z}_t=\sigma(\mathbf{X}_t\mathbf{W}_{xz}+\mathbf{H}_{t-1}\mathbf{W}_{hz}+\mathbf{b}_z)$

其中$\mathbf{W}_{xr}, \mathbf{W}_{xz} \in \mathbb{R}^{d \times h}$和$\mathbf{W}_{hr}, \mathbf{W}_{hz} \in \mathbb{R}^{h \times h}$为权重参数，$\mathbf{b}_r, \mathbf{b}_z \in \mathbb{R}^{1 \times h}$为偏置参数。

## 候选隐状态

![](https://i-blog.csdnimg.cn/direct/3aca9f579a4c4f5cbddc77b867766e9a.png)


接下来我们将重置门$\mathbf R_t$应用到常规RNN的隐状态更新公式中，得到的隐状态被称为**候选隐状态**：

$\widetilde{\mathbf{H}}_t = \tanh(\mathbf{X}_t \mathbf{W}_{xh} + \left(\mathbf{R}_t \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{hh} + \mathbf{b}_h)$

其中$\mathbf{W}_{xh} \in \mathbb{R}^{d \times h}$和$\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf{b}_h \in \mathbb{R}^{1 \times h}$是偏置参数（关于参数及其维度均可由字母及输入输出维度关系推得，此后不再赘述），符号$\odot$是按元素乘法。激活函数$\tanh$将输出压缩到区间$(-1,1)$中，既能避免数值爆炸，又能通过正负号增强信息表达。

$\mathbf R_t$中的每个元素都在区间$(0,1)$中，当其与$\mathbf{H}_{t-1}$中对应的元素相乘时，可以决定保留该元素的比例。它筛选过去的经验来作为理解新信息的参考。

## 隐状态

![](https://i-blog.csdnimg.cn/direct/e3c0dd07fb4a4059a19a9d61b61a368b.png)


最后我们使用更新门$\mathbf Z_t$将旧的隐状态$\mathbf H_{t-1}$（对过去信息的记忆）和候选隐状态$\widetilde{\mathbf{H}}_t$（对新信息的理解）加权求和，得到最终的隐状态（同时也是输出）：

$\mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t-1}  + (1 - \mathbf{Z}_t) \odot \widetilde{\mathbf{H}}_t$

当$\mathbf Z_t$接近$1$时，模型倾向于忽略新的信息，保留旧的记忆，从而跳过依赖链中的当前时间步；当$\mathbf Z_t$接近$0$时，模型倾向于让新的信息覆盖旧的记忆。该方法与ResNet的残差连接有异曲同工之处。

总而言之，重置门有助于捕获序列中的短期依赖关系，更新门有助于捕获序列中的长期依赖关系。这使得模型能更灵活地处理依赖关系的同时，还通过对信息的筛选降低了梯度传播路径的复杂性，避免其被不必要的分支稀释，缓解了梯度消失和梯度爆炸的问题。

# 长短期记忆网络（LSTM）

**长短期记忆网络**（LSTM）比GRU出现得更早，但设计更为复杂。

![](https://i-blog.csdnimg.cn/direct/04b924c6a8324c1bac942976f86f4323.png)


LSTM在隐状态之外引入了另一条在神经元之间传递的信息流，称为**记忆元**。可以将隐状态（同时也是输出）理解为短期记忆或对当前预测有用的信息，而记忆元则代表长期记忆。除此之外，LSTM包含**遗忘门**、**输入门**和**输出门**三个门控和**候选记忆元**计算，其中遗忘门、输入门和候选记忆元负责从短期记忆提炼信息以更新长期记忆，而输出门负责捕获长期依赖应用于当前预测。

## 遗忘门

![](https://i-blog.csdnimg.cn/direct/d54a613e70a542d786be0fea545bd446.png)


对于在时间步$t$的小批量输入$\mathbf{X}_t \in \mathbb{R}^{n \times d}$（$n$个样本，每个样本$d$个输入），前一个时间步的隐状态$\mathbf H_{t-1}\in\mathbb R^{n\times h}$（$h$个隐藏单元），遗忘门$\mathbf{F}_t \in \mathbb{R}^{n \times h}$的计算如下：

$\mathbf{F}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xf} + \mathbf{H}_{t-1} \mathbf{W}_{hf} + \mathbf{b}_f)$

和GRU同样地，sigmoid激活函数将其压缩到区间$(0,1)$得到一个比例系数，它通过乘法作用于长期记忆，决定应该遗忘哪些信息（$\mathbf{C}_{t0}$为博主自行用于表示记忆元计算的中间状态的变量名）：

$\mathbf{C}_{t0} = \mathbf{F}_t \odot \mathbf{C}_{t-1}$

## 输入门和候选记忆

![](https://i-blog.csdnimg.cn/direct/5aacda79b62042ac9971e8945459cc20.png)


候选记忆元$\widetilde{\mathbf{C}}_t \in \mathbb{R}^{n \times h}$和GRU的候选隐状态类似，表征一种在短期记忆基础上对新信息的理解，其计算如下：

$\widetilde{\mathbf{C}}_t = \text{tanh}(\mathbf{X}_t \mathbf{W}_{xc} + \mathbf{H}_{t-1} \mathbf{W}_{hc} + \mathbf{b}_c)$

输入门$\mathbf{I}_t \in \mathbb{R}^{n \times h}$则作为候选记忆元的比例系数，决定了对新信息的哪些理解可以进入长期记忆，其计算如下：

$\mathbf{I}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xi} + \mathbf{H}_{t-1} \mathbf{W}_{hi} + \mathbf{b}_i)$

将输入门作用于候选记忆元，得到的信息流入长期记忆，我们有这一阶段记忆元的更新公式如下：

$\mathbf{C}_t = \mathbf{C}_{t0} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t= \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t$

## 输出门

![](https://i-blog.csdnimg.cn/direct/9ea0e36949d64ebc9cd729e6b9ba73eb.png)


输出门用于捕获长期记忆中对当前预测较为关键的依赖信息并更新短期记忆，其计算如下：

$\mathbf{O}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xo} + \mathbf{H}_{t-1} \mathbf{W}_{ho} + \mathbf{b}_o)$

记忆元在被输出门筛选得到下一个隐状态之前，还会经过tanh激活函数以防数值爆炸：

$\mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t)$

现在，我们将LSTM的计算过程总结如下：

$\mathbf{F}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xf} + \mathbf{H}_{t-1} \mathbf{W}_{hf} + \mathbf{b}_f)$

$\mathbf{I}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xi} + \mathbf{H}_{t-1} \mathbf{W}_{hi} + \mathbf{b}_i)$

$\mathbf{O}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xo} + \mathbf{H}_{t-1} \mathbf{W}_{ho} + \mathbf{b}_o)$

$\widetilde{\mathbf{C}}_t = \text{tanh}(\mathbf{X}_t \mathbf{W}_{xc} + \mathbf{H}_{t-1} \mathbf{W}_{hc} + \mathbf{b}_c)$

$\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t$

$\mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t)$

总而言之，LSTM将长短期记忆分流处理、相互作用，短期记忆帮助长期记忆选择性遗忘和记忆，而长期记忆帮助短期记忆重新捕获和当下有关的长期依赖关系。短期记忆涉及了更多的参数，但数值范围被压缩以防数值爆炸；长期记忆的数值没有限制，但涉及的运算更为简洁。LSTM虽然比GRU复杂，但二者在大多数任务上的表现差不多。

对于序列中过长距离的依赖，LSTM和GRU的训练成本都是相当高的，在下一章将介绍更高级的替代模型Transformer。

# 深度循环神经网络（DRNN）

上一章只讨论了单隐藏层的RNN。通过将多层RNN堆叠在一起，我们可以得到具有更强表达能力和特征抽象能力的深度RNN（DRNN）。

![](https://i-blog.csdnimg.cn/direct/b8b0f438cd7449168e0bf1f9e8be2be9.png)


在深度RNN的隐藏层中，每一层接收来自上一层的隐状态和自己上一个时间步的隐状态来更新自己当前时间步的隐状态（除了第一层），又将自己更新后的隐状态作为输入传给下一层和下一个时间步的自己。

对于在时间步$t$的小批量输入$\mathbf{H}_t^{(0)} = \mathbf{X}_t \in \mathbb{R}^{n \times d}$（$n$个样本，每个样本$d$个输入），设第$l$个隐藏层（$l=1,\cdots,L$）的隐状态为$\mathbf{H}_t^{(l)} \in \mathbb{R}^{n \times h}$（$h$个隐藏单元），使用的激活函数为$\phi_l$，且设输出层变量为$\mathbf{O}_t \in \mathbb{R}^{n \times q}$（$q$个输出单元），则每个隐藏层的隐状态计算如下：

$\mathbf{H}_t^{(l)} = \phi_l(\mathbf{H}_t^{(l-1)} \mathbf{W}_{xh}^{(l)} + \mathbf{H}_{t-1}^{(l)} \mathbf{W}_{hh}^{(l)}  + \mathbf{b}_h^{(l)})$

最终输出层的计算为：

$\mathbf{O}_t = \mathbf{H}_t^{(L)} \mathbf{W}_{hq} + \mathbf{b}_q$

其中隐藏层数$L$和每层的隐藏单元数$h$都是超参数。此外，将LSTM和GRU计算隐状态的公式（输入替换为上一层的输出）即可实现深层LSTM和深层GRU，其中深层LSTM的记忆单元仍在层内传递。

# 双向循环神经网络（BiRNN）

在序列学习中，我们以往假定的目标是在给定观测的情况下对下一个输出进行预测。但除此之外，还有一种任务是根据上下文预测序列中间空缺的信息，这使得模型需要捕捉过去和未来两个方向上的依赖关系，而目前为止，RNN只会观测过去的信息。

![](https://i-blog.csdnimg.cn/direct/b894dd11216b463e82d533f4c9590c4a.png)


双向RNN（BiRNN）在传统RNN的基础上添加了反向传递信息的隐藏层。输入会分别进入前向隐藏层和反向隐藏层进行处理，而输出层则将前向隐状态和反向隐状态连接起来进行计算。

对于在时间步$t$的小批量输入$\mathbf{X}_t \in \mathbb{R}^{n \times d}$（$n$个样本，每个样本$d$个输入），设该时间步的前向隐状态和反向隐状态分别为$\overrightarrow{\mathbf{H}}_t \in \mathbb{R}^{n \times h}$和$\overleftarrow{\mathbf{H}}_t \in \mathbb{R}^{n \times h}$（$h$个隐藏单元，两个方向上的数目可以是不同的），则其计算如下：

$\overrightarrow{\mathbf{H}}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(f)} + \overrightarrow{\mathbf{H}}_{t-1} \mathbf{W}_{hh}^{(f)} + \mathbf{b}_h^{(f)})$

$\overleftarrow{\mathbf{H}}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(b)} + \overleftarrow{\mathbf{H}}_{t+1} \mathbf{W}_{hh}^{(b)}  + \mathbf{b}_h^{(b)})$

参数的上标$(f)$和$(b)$分别表示前向和反向。

将前向隐状态$\overrightarrow{\mathbf{H}}_t$和反向隐状态$\overleftarrow{\mathbf{H}}_t$连接得到进入输出层（在深度BiRNN中进入下一个双向隐藏层）的隐状态$\mathbf{H}_t \in \mathbb{R}^{n \times 2h}$，最终输出层的输出$\mathbf{O}_t \in \mathbb{R}^{n \times q}$（$q$个输出单元）计算如下：

$\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q$

由于BiRNN需要同时进行前向和反向的隐状态计算，因此需要完整的序列才能计算每一个时间步的输出。在填充序列空缺任务中，其预测值为对应时间步的输出，而在序列标注任务中，最终的输出会结合所有时间步的输出给出。

BiRNN不适用于单向预测，因为这样的测试缺少在训练中能够利用的未来的信息，大大降低了模型的精确度。还有一个严重的问题是，双向递归使得梯度求解的链条变得非常长，导致模型的计算速度非常慢。

# 序列到序列（Seq2Seq）

在一类重要的自然语言处理任务中，输入和输出都是不定长的序列，例如机器翻译和对话，它们被称为**序列到序列**（Seq2Seq）类学习任务，这些是**序列转换模型**的核心问题。

## 编码器-解码器架构

为了处理上述类型的输入和输出，深度学习引入了通信领域中编码与解码的概念。为了避免信息在传输过程中被干扰失真，在传输前需要先将信号编码为易于传输的形式，然后在接收时将信号解码还原。

Seq2Seq使用包含两个主要组件的架构。第一个组件是**编码器**，它接收一个长度可变的序列作为输入，并将其转换为具有固定形状的编码状态；第二个组件是**解码器**，它将固定形状的编码状态映射到长度可变的序列。这被称为**编码器-解码器**架构。

![](https://i-blog.csdnimg.cn/direct/e11883ad4edc42eeabf05563b9628b51.png)


### 编码器

编码器将长度可变的输入序列转换成形状固定的上下文变量$\mathbf c$，并将输入序列的信息在该上下文变量中进行编码。以用RNN设计的编码器为例，考虑单个样本，在编码之前，RNN先遍历序列$\{\mathbf x_t\}$计算出每一个时间步$t$的隐状态$\mathbf h_t$：

$\mathbf{h}_t = f(\mathbf{x}_t, \mathbf{h}_{t-1})$

编码过程则以所有时间步的隐状态作为输入，通过选定的函数$q$将其转换为上下文变量：

$\mathbf{c} =  q(\mathbf{h}_1, \cdots, \mathbf{h}_T)$

上下文变量是从输入序列中提取出的特征表示，一种简单的选择是令$\mathbf{c} = \mathbf{h}_T$，直接使用最后时间步的隐状态作为上下文变量。

### 解码器

解码器使用来自编码器的上下文变量初始化自己的隐状态，并作为之后输出的参考。假设使用另一个RNN作为解码器，对于解码过程，在输出序列的时间步$t'$，解码器将连接自己上一个时间步的输出$\mathbf y_{t'-1}$和上下文变量$\mathbf c$作为输入来更新自己上一个时间步的隐状态$\mathbf s_{t'-1}$（使用$\mathbf s$与解码器的隐状态区分开）为$\mathbf s_{t'}$：

$\mathbf{s}_{t^\prime} = g(y_{t^\prime-1}, \mathbf{c}, \mathbf{s}_{t^\prime-1})$

随后解码器通过输出层计算在时间步$t'$的输出$\mathbf y_{t'}$，这个过程也是将隐状态映射为词表大小的矩阵，以便对其进行softmax操作转化成各个词元作为预测结果的条件概率，最终给出条件概率最大的词元。只要给解码器一个初始的输入$\mathbf y_1$，解码器就能通过对后续时间步的预测输出序列，直到其判断序列可以终止。

![](https://i-blog.csdnimg.cn/direct/9fd9c5a6101447e0b599aac504fbf8f0.png)


上图给出了在机器翻译中编码器-解码器架构是如何工作的，其中$\texttt{<bos>}$和$\texttt{<eos>}$分别为序列开始词元和序列结束词元。编码器提炼待翻译序列的信息送往解码器，解码器接收$\texttt{<bos>}$作为起点，根据编码信息和已翻译出的内容继续预测翻译内容的下一个词元，直到预测出$\texttt{<eos>}$作为翻译内容的结束。而在训练过程中，解码器会直接接收标签序列作为输入以从中进行学习。

## 预测序列的评估

对于以可变长度序列为输出的预测，我们可以通过与真实的标签序列进行比较来评估。**BLEU**（Bilingual Evaluation Understudy）最先被用于评估机器翻译的结果，但现在已经被广泛用于度量输出序列的质量。其定义如下：

$\mathrm{BLEU}=\exp\left(\min\left(0, 1 - \frac{\mathrm{len}_{\text{label}}}{\mathrm{len}_{\text{pred}}}\right)\right) \displaystyle\prod_{n=1}^k p_n^{1/2^n}$

从右往左看，$p_n$表示$n$元语法（连续$n$个词元组成的序列）的精确率，其计算步骤如下：

1. 分别统计标签序列和预测序列中的所有$n$元语法（包括重复出现的项）；
2. 将标签序列的$n$元语法与预测序列的$n$元语法一一匹配，每项只能被匹配一次，统计匹配次数；
3. 将匹配次数除以预测序列中$n$元语法的总数（包括重复出现的项）。

例如对于标签序列$\{A,B,C,C,D\}$和预测序列$\{A,B,B,C\}$，$p_1=\displaystyle\frac34$，$\displaystyle p_2=\frac23$，$p_3=p_4=0$。

由于$n$越大，$n$元语法的匹配难度也越大，$\mathrm{BLEU}$通过$p_n^{1/2^n}$为更长的$n$元语法的精确率赋予了更大的权重（底数小于$1$时，指数越小幂越大），再将所有的精确率累乘。

最后，越短的预测序列，其$n$元语法的总数越小，获得的$p_n$越大，因此$\mathrm{BLEU}$通过系数$\exp\left(\min\left(0, 1 - \frac{\mathrm{len}_{\text{label}}}{\mathrm{len}_{\text{pred}}}\right)\right)$对其进行惩罚。当预测序列长度小于标签序列时，$e$的负数次幂会降低评估结果。而当预测序列长度大于标签序列时，精确率则会下降。只有当预测序列与标签序列完全相同时，$\mathrm{BLEU}=1$。

# 束搜索

本节讨论解码器在选择词元作为输出时的策略问题。

## 贪心搜索

在上一节中，解码器直接在所有词元中选择条件概率最大的作为输出，即采取**贪心搜索**：

$y_{t'} = \operatorname*{argmax}_{y \in \mathcal{Y}} P(y \mid y_1, \ldots, y_{t'-1}, \mathbf{c})$

一旦输出序列包含了$\texttt{<eos>}$或到达其最大长度$T'$，则输出完成。

这种策略带来的问题是，在当前时间步条件概率最大的词元，长远来看，在其后的几个时间步中，以该词元为起点的预测序列并不一定是所有预测序列中条件概率最高的，即局部最优不一定带来总体最优。

![](https://i-blog.csdnimg.cn/direct/c6feacc8b2314b1b98bb56752fbca7a5.png)


在上图所示的例子中，横向为不同时间步，纵向为不同词元。左侧使用贪心算法得到的预测序列为ABC，其概率为$0.5\times0.4\times0.4\times0.6=0.048$，而右侧在第 $2$个时间步未使用贪心算法，得到的预测序列为ACB ，其概率为$0.5\times0.3\times0.6\times0.6=0.054$（第$3$、$4$个时间步的概率分布不一样是因为第$2$个时间步选择的概率条件不一样），优于贪心算法。

## 穷举搜索

如果想获得最优序列，我们可以考虑**穷举搜索**，即穷举所有可能的输出序列及其条件概率，计算输出条件概率最大的那一个。

穷举搜索虽然能保证选择的最优性，但其计算量$O(|Y|^{T'})$大得惊人（$|Y|$为词表大小），在计算机上运行是几乎不可能的。

## 束搜索

**束搜索**介于贪心搜索和穷举搜索之间，兼顾精确度和计算成本。

![](https://i-blog.csdnimg.cn/direct/91de3a00156c49c39e3e9c38cd4225a9.png)


束搜索在每个时间步$t$做选择时，会同时选中条件概率最大的$k$个词元作为候选输出，超参数$k$是束搜索的**束宽**。在下一个时间步，从这$k$个词元衍生出来的所有分支总共有$k|Y|$个选择（$|Y|$为词表大小），而束搜索会继续同时选中这所有$k|Y|$个选择中条件概率最大的$k$个词元，以此类推。在预测的过程中，束搜索将始终保持有$k$个候选输出序列，如果一条候选序列遇到$\texttt{<eos>}$则停止延伸，剩余可扩展的候选序列数量会相应地减少。所有候选序列均完成后则终止束搜索。

最后我们将从最终候选输出序列集合中选择分数最高的序列作为输出序列，每个候选的分数计算如下：

$\displaystyle\frac{1}{L^\alpha} \log P(y_1, \ldots, y_{L}\mid \mathbf{c}) = \frac{1}{L^\alpha} \sum_{t'=1}^L \log P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c})$

由于条件概率由概率累乘而来，容易产生数值下溢，因此在计算时取对数处理。除此之外，在候选序列中，长序列的条件概率会显著小于短序列，为了给予补偿，我们使用系数$\displaystyle\frac1{L^\alpha}$，其中$L$为序列长度，$\alpha$一般取$0.75$。对数运算结果为负数，对于更大的$L$，该系数使最终分数更大。

束搜索的计算量为$O(k|Y|T')$，即每个时间步需要遍历$k|Y|$项，其计算成本介于贪心搜索和穷举搜索之间。通过灵活地选择束宽，束搜索可以在精确度和计算成本之间权衡。
