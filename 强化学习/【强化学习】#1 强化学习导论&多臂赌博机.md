> 主要参考学习资料：《强化学习（第2版）》[加]Richard S.Suttion [美]Andrew G.Barto 著

**概述**

- **强化学习**通过与环境交互来学习，其面对的问题是**马尔可夫决策过程**。
- 强化学习的要素有**策略、收益信号、价值函数和环境模型**。
- 强化学习的一个独有挑战是**试探-开发困境**。
- **k臂赌博机**是强化学习问题的一种极简情况。
- 选择动作的依据可以是**价值估计、置信度上界和偏好函数**。
- 对于价值估计，**采样平均方法**可用于平稳问题，**指数近因加权平均**可用于非平稳问题。而**增量式实现**可以高效计算样本均值。

@[TOC](目录)

# 导论
## 强化学习

人类通过与环境交互来学习，在交互中学习的计算性方法被称为**强化学习**。

强化学习就是学习在当前的情境采取什么动作才能使得数值化的收益信号最大化，动作往往影响的不仅仅是即时收益，也会影响下一个情境，从而影响随后的收益。**试错**和**延迟收益**是强化学习两个最重要最显著的特征。

强化学习解决的问题是**马尔可夫决策过程**。具有学习能力的智能体必须能够在某种程度上感知环境的状态，然后采取动作并影响环境状态。智能体必须同时拥有和环境状态相关的一个或多个明确的目标。马尔可夫决策过程即包含了**感知、动作和目标**三方面。

强化学习与有监督学习不同，我们不可能获得在所有情境（特征）下既正确又有代表性的动作示例（标签）。在一个未知领域，若想收益最大，智能体必须要能够从自身的经验中学习。

强化学习也与无监督学习不同，其目的是最大化收益信号，而不是找出数据的隐含结构。

强化学习的一个独有挑战是“试探-开发”困境问题。智能体必须**开发**已有的经验来获取收益，同时也要进行**试探**，使得未来可以获得更好的动作选择空间。但无论试探与开发，都不能在完全没有失败的情况下进行，此时必须在两者之间进行权衡。

以下是一些体现强化学习思想的案例：

- 国际象棋大师走一步棋。这个选择是通过反复计算对手可能的策略和对特定局面位置及走棋动作的直观判断做出的。
- 一只羚羊幼崽出生后数分钟挣扎着站起来。半小时后，它能够以每小时20英里的速度奔跑。
- 一个移动机器人决定它是进入一个新房间收集更多垃圾还是返回充电站充电。它的决定将基于当前电量，以及它过去走到充电站的难易程度。

## 强化学习要素

除了智能体和环境，强化学习系统有四个核心元素：**策略、收益信号、价值函数**以及（可选的）对环境建立的**模型**。

**策略**定义了学习智能体在特定时间的行为方式，是环境状态到动作的映射。一般来说，策略可能是环境所在状态和智能体所采取的动作的随机函数。

**收益信号**定义了强化学习问题中的目标。在每一步中，环境向强化学习智能体发送一个称为**收益**的标量数值，智能体的唯一目标是最大化长期总收益。一般来说，收益信号可能是环境状态和在此基础上所采取的动作的随机函数。

收益信号表明了在短时间内什么是好的，**价值函数**表示了从长远的角度看什么是好的。一个状态的**价值**是一个智能体从这个状态开始，对将来累积的总收益的期望。我们寻求能带来最高价值而不是最高收益的动作，但确定价值比确定收益难得多。

对环境建立的**模型**是一种对环境的反应模式的模拟，对外部环境的行为进行推断。环境模型会被用于**规划**。规划是在真正经历之前，先考虑未来可能发生的各种情境从而预先决定采取何种动作。使用环境模型和规划来解决强化学习问题的方法被称为**有模型的方法**，而简单的**无模型的方法**则是直接地试错。

# 多臂赌博机

## 一个k臂赌博机问题

**k臂赌博机问题**是一个不存在状态信息，只有动作与奖励的简化情况，其原始形式如下：你要重复地在$k$个选项或动作中进行选择。每次做出选择之后，你都会得到一定数值的收益，收益由你选择的动作决定的平稳概率分布产生。你的目标是在某一段时间内最大化收益期望。

$k$个动作中每一个在被选择时都有一个期望或者平均收益，我们称此为这个动作的**价值**。我们将在时刻$t$选择的动作记作$A_t$，并将对应的收益记作$R_t$。任一动作对应的价值记作$q_*(a)$，是给定动作$a$时收益的期望：

$q_*(a)\dot=\mathbb E[R_t|A_t=a]$

通常我们不知道动作的价值，但可以估计。我们将对时刻$t$的价值的估计记作$Q_t(a)$，我们希望它接近$q^*(a)$。

在任一时刻，至少有一个动作的估计价值是最高的，这些对应估计最高价值的动作被称为**贪心**的动作。从贪心动作中选择是**开发**当前你所知道的关于动作的价值的知识，从非贪心动作中选择则是**试探**，可以改善对非贪心动作的价值的估计。“开发”对于最大化当前这一时刻的期望收益是正确的做法，而“试探”从长远来看可能会带来总体收益最大化。在同一次动作选择中，开发和试探不可能同时进行，此为开发和试探之间的冲突。

## 动作-价值方法

使用价值的估计来进行动作的选择的方法被统称为**动作-价值方法**。一种自然的方式就是通过计算实际收益的平均值来估计动作的价值：

$Q_t(a)=\displaystyle\frac{\displaystyle\sum^{t-1}_{i=1}R_i\mathbb 1_{A_i=a}}{\displaystyle\sum^{t-1}_{i=1}\mathbb 1_{A_i=a}}$

其中$\mathbb 1_{\mathrm {predicate}}$表示随机变量，当$\mathrm{predicate}$（此处为时刻$i$选择了动作$a$）为真时其值为$1$，反之为$0$。当分母为$0$时，我们将$Q_t(a)$定义为某个默认值，比如$Q_t(a)=0$。当分母趋向无穷大时，根据大数定律，$Q_t(a)$会收敛到$q^*(a)$。这种估计动作价值的方法称为**采样平均方法**。

最简单的动作选择规则是选择贪心动作，如果有多个贪心动作就任选一个：

$A_t\dot=\underset{a}{\mathrm{argmax}}Q_t(a)$

选择贪心动作总是利用当前的知识最大化眼前的收益，不花时间尝试非贪心动作是否会更好。

贪心策略的一个简单替代策略是大部分时间都表现得贪心，但以一个很小的概率$\epsilon$从所有动作中等概率随机做出选择，称为**ε-贪心方法**。其优点是如果时刻无限长，则每一个动作都会被无限次采样，从而确保所有的$Q_t(a)$收敛到$q^*(a)$，但这只是渐近性的保证。

$\epsilon$-贪心方法相对于贪心方法的优点依赖于任务。当收益的方差更大时，由于收益的噪声更多，找到最优的动作所需的试探次数也更多。除此之外，如果动作的真实价值会随着时间而变化，试探也是需要的。

## 增量式实现

下面探讨如何高效计算收益的样本均值。对于单个动作，令$R_i$表示这一动作被选择$i$次后获得的收益，$Q_n$表示被选择$n-1$次后它的估计的动作价值，则：

$Q_n\dot=\displaystyle\frac{R_1+R_2+\cdots+R_{n-1}}{n-1}$

该实现需要维护所有收益的记录，并在每次估计时计算，内存和计算量会随着时间增长。

设计增量式公式可以以小而恒定的计算更新平均值。给定$Q_n$和第$n$次的收益$R_n$，新的均值可以这样计算：

$$\begin{equation}\begin{split}Q_{n+1}&=\displaystyle\frac1n\sum^n_{i=1}R_i\\&=\displaystyle\frac1n\left(R_n+\sum^{n-1}{i=1}R_i\right)\\&=\displaystyle\frac1n\left(R_n+(n-1)\frac1{n-1}\sum^{n-1}_{i=1}R_i\right)\\&=\displaystyle\frac1n\left(R_n+(n-1)Q_n\right)\\&=\displaystyle Q_n+\frac1n\left(R_n-Q_n\right)\end{split}\end{equation}$$

更新公式的形式将会贯穿强化学习始终，其一般形式是：

$新估计值\leftarrow旧估计值+步长\times[目标-旧估计值]$

$[目标-旧估计值]$是估计值的**误差**，误差会随着向目标靠近的每一步而减小。

每个动作的步长会随着时间而变化，我们将其记作$\alpha_t(a)$。

假设函数$\mathit{bandit}(a)$接收一个动作作为参数并返回一个对应的收益，$N(A)$为动作被选择的次数，一个使用增量式计算样本均值和$\epsilon$-贪心动作选择的赌博机问题算法的伪代码如下：

>  初始化，令$a=1$到$k$：
> 
> $Q(a)\leftarrow0$
> 
> $N(a)\leftarrow0$
> 
> 无限循环：
> 
> $A\leftarrow\left\{\begin{matrix}\underset{a}{\mathrm{argmax}}Q(a)&以1-\epsilon概率\\一个随机的动作&以\epsilon概率\end{matrix}\right.$
> 
> $R\leftarrow\mathit{bandit}(A)$
> 
> $N(A)\leftarrow N(A)+1$
> 
> $Q(A)\leftarrow Q(A)+\displaystyle\frac1{N(A)}[R-Q(A)]$

## 非平稳问题

在收益概率随时间变化的非平稳问题下，上述取平均方法不再合适，给近期的收益赋予比过去很久的收益更高的权值是一种更合理的处理方式，最流行的方法之一是使用固定步长：

$Q_{n+1}\dot=Q_n+\alpha\left[R_n-Q_n\right]$

式中，步长参数$\alpha\in(0,1]$是一个常数，这使得$Q_{n+1}$成为过去的收益和初始的估计$Q_1$的加权平均：

$$\begin{equation}\begin{split}Q_{n+1}&=Q_n+\alpha[R_n-Q_n]\\&=\alpha R_n+(1-\alpha)Q_n\\&=\alpha R_n+(1-\alpha)[\alpha R_{n-1}+(1-\alpha)Q_{n-1}]\\&=\alpha R_n+(1-\alpha)\alpha R_{n-1}+(1-\alpha)^2Q_{n-1}\\&=(1-\alpha)^nQ_1+\displaystyle\sum^n_{i=1}\alpha(1-\alpha)^{n-i}R_i\end{split}\end{equation}$$

其中$1-\alpha<1$，因此赋予$R_i$的权值$\alpha(1-\alpha)^{n-i}$随着相隔次数的增加以指数形式递减，因此该方法也被称为**指数近因加权平均**。

大数定律的收敛性不能保证对任何$\{\alpha_n(a)\}$序列都满足，随机逼近理论给出了保证收敛概率为$1$所需的条件：

$\displaystyle\sum^\infty_{n=1}\alpha_n(a)=\infty且\sum^\infty_{n=1}\alpha^2_n(a)<\infty$

第一个条件保证有足够大的步长最终克服任何初始条件或随机波动，第二个条件保证最终步长变小以保证收敛。采样平均方法满足两个收敛条件，而常数步长参数不满足第二个收敛条件，这正是非平稳环境所需的，而且强化学习的问题常常是非平稳的。

## 乐观初始值

目前我们讨论的所有方法都在一定程度上依赖于初始动作值$Q_1(a)$的选择。其缺点是初始估计值如果不都设为$0$，则变成了必须由用户选择的参数集，好处是通过它们可以简单地设置关于预期收益水平的先验知识。

初始动作的价值也提供了一种简单的试探方式。如果将所有动作都设置一个相比实际期望较高的初始值，那么这种乐观的初始估计会鼓励动作-价值方法去试探，因为无论哪一种动作被选择的收益都比最开始的估计值小，学习器会感到“失望”而转向另一个动作。这种鼓励试探的技术叫做**乐观初始价值**。

但这种方法不适合非平稳问题，因为其试探的驱动力是暂时的。任何仅仅关注初始条件的方法都不太可能对一般的非平稳情况有所帮助，但是它们都很简单，其中一个或几个简单的组合在实践中往往是足够的。

## 基于置信度上界的动作选择

在非贪心动作中，最好是根据它们的潜力来选择可能事实上是最优的动作，而不是$\epsilon$-贪心算法的盲目选择。一个有效的方法是按照以下公式选择动作：

$A_t\dot=\underset a{\mathrm{argmax}}\left[Q_t(a)+c\displaystyle\sqrt{\frac{\ln t}{N_t(a)}}\right]$

其中$N_t(a)$表示在时刻$t$之前动作$a$被选择的次数，如果$N_t(a)=0$，则$a$就被认为是满足最大化条件的动作。

这种基于**置信度上界**（UCB）的动作选择的思想是，平方根项是对$a$动作值估计的不确定性或方差的度量。每次选择$a$会使这一项减小（$N_t(a)$增长大于$\ln t$）)，而选择$a$之外的动作会使这一项增大（$\ln t$增长，$N_t(a)$不变）。

但是和$\epsilon$-贪心算法相比，它更难推广到跟一般的学习问题，其一是因为它处理非平稳问题时需要更复杂的估计更新方法，其二是因为它要处理更大的状态空间，特别是在后续会介绍的函数近似问题中。

## 梯度赌博机算法

评估动作价值并使用估计值来选择动作并不是唯一可使用的方法。考虑到和绝对的收益相比，一个动作对另一个动作的相对偏好可能更为重要，于是我们引入softmax分布来计算动作$a$在时刻$t$的被选择概率：

$\pi_t(a)\dot=\displaystyle\frac{e^{H_t(a)}}{\displaystyle\sum^k_{b=1}e^{H_t(b)}}$

其中$H_t(a)$是为了计算上式引入的**偏好函数**，如果每一个动作的偏好函数都增加相同的值，对上式得到的动作概率没有任何影响。

基于随机梯度上升的思想，在每个步骤选择动作$a_i$并获得收益$R_t$之后，偏好函数按如下方式更新：

$H_{t+1}(a_i)\dot=H_t(a_i)+\alpha(R_t-\bar R_t)(1-\pi_t(a_i))$

$H_{t+1}(a_j)\dot=H_t(a_j)-\alpha(R_t-\bar R_t)\pi_t(a_j),对所有j\neq i$

关于其推导博主参考了和书中不一样的方法手推：

![](https://i-blog.csdnimg.cn/direct/ac4360fb1c7e49d0bd955cda1f9a85fe.jpeg)


$\theta$是包含所有动作的偏好函数的向量，目标函数是期望奖励，梯度上升的过程就是调节所有动作的偏好函数使期望奖励最大化。

由于我们无法遍历所有动作来计算期望，只能对每个步骤选择的动作$a_i$做单次采样近似：

![](https://i-blog.csdnimg.cn/direct/d0d6025701cc434cbc7b68e6bead4bdf.jpeg)


直接采用当前时刻观测到的奖励$r(a_i)$来更新梯度的问题是，其方差带来的波动会使$\theta$的更新不稳定，收敛速度变慢。为了削弱方差的影响，我们在此基础上减去一个较为稳定的基线$b$，当$b$选择过去收益的均值$\bar R_t$，并辅以一个超参数$\alpha$用于调控学习率，就得到了一开始给出的式子。

## 关联搜索

一般的强化学习任务中，往往有不止一种情境。以一个非平稳的$k$臂赌博机为例，我们为其添加外观颜色的情境，赌博机的外观颜色与其动作价值集合一一对应，并随着动作价值集合的改变而改变，那么学习器还需要学习外观颜色到动作价值集合的映射关系，将情境与最优动作关联起来，这便是**关联搜索**任务。

关联搜索任务介于$k$臂赌博机问题和完整强化学习问题之间。和$k$臂赌博机相比，它还需要学习一种**策略**。和完整强化学习问题相比，它的每个动作只影响即时收益。如果动作可以影响下一时刻的情境和收益，便是完整的强化学习问题，将在下一章具体提出。
